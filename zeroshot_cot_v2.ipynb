{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4880a2fd1d1c4668b9013b4ea16e38fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You shouldn't move a model that is dispatched using accelerate hooks.\n",
      "  0%|          | 0/869 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:452: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n",
      "100%|██████████| 869/869 [1:15:56<00:00,  5.24s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from ast import literal_eval\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "system_message = \"You are an AI assistant that answers multiple-choice questions by analyzing a given paragraph. Return only the answer number. Avoid any additional explanations.\"\n",
    "\n",
    "PROMPT_NO_QUESTION_PLUS = \"\"\"Solve the following multiple-choice Korean question. Provide the answer as a single number (1, 2, 3, 4, or 5). Example: Answer: 1\n",
    "\n",
    "Paragraph:\n",
    "{paragraph}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Choices:\n",
    "{choices}\n",
    "\n",
    "Instructions:\n",
    "1. Carefully read the paragraph to extract relevant information.\n",
    "2. Identify key points in the question and align them with the paragraph content.\n",
    "3. Analyze each choice in relation to the paragraph and eliminate incorrect options step by step.\n",
    "4. Briefly justify why each eliminated option is incorrect.\n",
    "5. Clearly state the correct answer, providing concise reasoning.\n",
    "\n",
    "Finally, provide the answer in the format 'Answer: [number]'. The answer must be one of 1, 2, 3, 4, or 5.\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_QUESTION_PLUS = \"\"\"Solve the following multiple-choice Korean question. Provide the answer as a single number (1, 2, 3, 4, or 5). Example: Answer: 1\n",
    "\n",
    "Paragraph:\n",
    "{paragraph}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Additional Information:\n",
    "{question_plus}\n",
    "\n",
    "Choices:\n",
    "{choices}\n",
    "\n",
    "Instructions:\n",
    "1. Start by understanding the paragraph's context and extract key details.\n",
    "2. Integrate the additional information provided to refine your understanding.\n",
    "3. Carefully analyze the question and determine its focus.\n",
    "4. Evaluate each choice in relation to the paragraph and additional information.\n",
    "5. Eliminate incorrect answers step by step, explaining why each one does not fit.\n",
    "6. Select the most appropriate choice and justify your decision concisely.\n",
    "\n",
    "Finally, provide the answer in the format 'Answer: [number]'. The answer must be one of 1, 2, 3, 4, or 5.\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Load Qwen-2.5-32B-instruct Model and Tokenizer\n",
    "model_name_or_path = \"Qwen/Qwen2.5-32B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path, trust_remote_code=True,\n",
    "    load_in_4bit=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path, trust_remote_code=True,\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def load_and_process_data(file_path):\n",
    "    dataset = pd.read_csv(file_path)\n",
    "    \n",
    "    records = []\n",
    "    for _, row in dataset.iterrows():\n",
    "        problems = literal_eval(row['problems'])\n",
    "        record = {\n",
    "            'id': row['id'],\n",
    "            'paragraph': row['paragraph'],\n",
    "            'question': problems['question'],\n",
    "            'choices': problems['choices'],\n",
    "            'answer': problems.get('answer', None),\n",
    "            \"question_plus\": problems.get('question_plus', None),\n",
    "            'explanation': row.get('explanation', None)\n",
    "        }\n",
    "        records.append(record)\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    df['question_plus'] = df['question_plus'].fillna('')\n",
    "    return Dataset.from_pandas(df)\n",
    "\n",
    "def format_inference_dataset(test_df):\n",
    "    test_dataset = []\n",
    "    for i, row in test_df.iterrows():\n",
    "        choices_string = \"\\n\".join([f\"{idx + 1} - {choice}\" for idx, choice in enumerate(row[\"choices\"])])\n",
    "        len_choices = len(row[\"choices\"])\n",
    "        \n",
    "        if row[\"question_plus\"]:\n",
    "            user_message = PROMPT_QUESTION_PLUS.format(\n",
    "                paragraph=row[\"paragraph\"],\n",
    "                question=row[\"question\"],\n",
    "                question_plus=row[\"question_plus\"],\n",
    "                choices=choices_string,\n",
    "            )\n",
    "        else:\n",
    "            user_message = PROMPT_NO_QUESTION_PLUS.format(\n",
    "                paragraph=row[\"paragraph\"],\n",
    "                question=row[\"question\"],\n",
    "                choices=choices_string,\n",
    "            )\n",
    "\n",
    "        test_dataset.append(\n",
    "            {\n",
    "                \"id\": row[\"id\"],\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": system_message},\n",
    "                    {\"role\": \"user\", \"content\": user_message},\n",
    "                ],\n",
    "                \"label\": row[\"answer\"],\n",
    "                \"len_choices\": len_choices,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return test_dataset\n",
    "\n",
    "model.eval()\n",
    "model.to(\"cuda\")\n",
    "\n",
    "test_df = load_and_process_data('test.csv').to_pandas()\n",
    "test_dataset = format_inference_dataset(test_df)\n",
    "\n",
    "infer_results = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for data in tqdm(test_dataset, total=len(test_dataset)):\n",
    "        input_tensor = tokenizer.apply_chat_template(\n",
    "            data[\"messages\"],\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        outputs = model.generate(\n",
    "            input_tensor,\n",
    "            max_new_tokens=450,\n",
    "            pad_token_id=tokenizer.eos_token_id,   \n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=False,\n",
    "            temperature=None, \n",
    "            top_p=None,\n",
    "            top_k=None,\n",
    "        )\n",
    "\n",
    "        response = tokenizer.decode(outputs[0][input_tensor.size(1):], skip_special_tokens=True)\n",
    "\n",
    "        infer_results.append({\n",
    "            \"id\": data[\"id\"],\n",
    "            \"answer\": response.strip()\n",
    "        })\n",
    "\n",
    "pd.DataFrame(infer_results).to_csv(\"output_zeroshot_cot_v2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.7883\n",
      "Accuracy for 434 random rows (seed=42): 0.7765\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the uploaded CSV file\n",
    "file_path = 'output_zeroshot_cot_v2.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Function to clean and convert 'answer' values\n",
    "def process_answer(value):\n",
    "    try:\n",
    "        # Extract numeric value after \"Answer:\" if present, else use the value directly\n",
    "        if isinstance(value, str) and \"Answer:\" in value:\n",
    "            numeric_value = int(value.split(\"Answer:\")[-1].strip())\n",
    "        else:\n",
    "            numeric_value = int(value)\n",
    "        \n",
    "        # Ensure the numeric value is within 1-5\n",
    "        if 1 <= numeric_value <= 5:\n",
    "            return numeric_value\n",
    "        else:\n",
    "            return 1  # Default to 1 if out of range\n",
    "    except:\n",
    "        return 1  # Default to 1 if conversion fails\n",
    "\n",
    "# Apply the processing function to the 'answer' column\n",
    "df['answer'] = df['answer'].apply(process_answer)\n",
    "\n",
    "# Save the processed DataFrame\n",
    "output_path = 'processed_output_zeroshot_cot_v2.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "\n",
    "# File paths\n",
    "test_path = '../data/test_answer_gpt4o.csv'\n",
    "predictions_path = 'processed_output_zeroshot_cot_v2.csv'\n",
    "\n",
    "# Load data\n",
    "test = pd.read_csv(test_path)\n",
    "predictions = pd.read_csv(predictions_path)\n",
    "\n",
    "# Calculate overall accuracy\n",
    "accuracy = (test['answer'] == predictions['answer']).mean()\n",
    "print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Accuracy for 434 rows sampled with random seed 42\n",
    "sampled_indices = test.sample(n=434, random_state=42).index\n",
    "sampled_accuracy = (test.loc[sampled_indices, 'answer'] == predictions.loc[sampled_indices, 'answer']).mean()\n",
    "print(f\"Accuracy for 434 random rows (seed=42): {sampled_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
