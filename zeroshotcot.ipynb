import torch
import numpy as np
import pandas as pd
from tqdm import tqdm
from ast import literal_eval
from transformers import AutoTokenizer, AutoModelForCausalLM
import re

# Updated Prompt Templates
PROMPT_NO_QUESTION_PLUS = """Please analyze and solve the following problem step by step. First, provide the answer as a single number (1, 2, 3, 4, or 5). Example: Answer: 1

Paragraph:
{paragraph}

Question:
{question}

Choices:
{choices}

First, write the answer in the format 'Answer: [number]', then analyze the problem step by step. The answer must be one of 1, 2, 3, 4, or 5.

Answer:
"""

PROMPT_QUESTION_PLUS = """Please analyze and solve the following problem step by step. First, provide the answer as a single number (1, 2, 3, 4, or 5). Example: Answer: 1

Paragraph:
{paragraph}

Question:
{question}

<Additional Information>:
{question_plus}

Choices:
{choices}

First, write the answer in the format 'Answer: [number]', then analyze the problem step by step. The answer must be one of 1, 2, 3, 4, or 5.

Answer:
"""

# Load Qwen-2.5-32B-instruct Model and Tokenizer
model_name_or_path = 'Qwen/Qwen2.5-32B-Instruct'
inference_csv = 'test.csv'
output_csv = 'output_zeroshot_cot.csv'

system_message = "당신은 지문을 읽고 객관식 문제를 단계별로 분석한 뒤 정답을 도출하는 AI 어시스턴트입니다. 지문을 분석하고 정답 번호만 반환하세요."

model = AutoModelForCausalLM.from_pretrained(
    model_name_or_path, 
    trust_remote_code=True,
    # device_map="auto",
    load_in_4bit=True
)
tokenizer = AutoTokenizer.from_pretrained(
    model_name_or_path, trust_remote_code=True,
)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

def load_and_process_data(file_path):
    dataset = pd.read_csv(file_path)
    records = []
    for _, row in dataset.iterrows():
        problems = literal_eval(row['problems'])
        record = {
            'id': row['id'],
            'paragraph': row['paragraph'],
            'question': problems['question'],
            'choices': problems['choices'],
            'answer': problems.get('answer', None),
            "question_plus": problems.get('question_plus', None),
        }
        records.append(record)
    df = pd.DataFrame(records)
    df['question_plus'] = df['question_plus'].fillna('')
    return df

def format_inference_dataset(test_df):
    test_dataset = []
    for _, row in test_df.iterrows():
        choices_string = "\n".join([f"{idx + 1} - {choice}" for idx, choice in enumerate(row["choices"])]).strip()
        
        # <보기>가 있을 때
        if row["question_plus"]:
            user_message = PROMPT_QUESTION_PLUS.format(
                paragraph=row["paragraph"],
                question=row["question"],
                question_plus=row["question_plus"],
                choices=choices_string,
            )
        # <보기>가 없을 때
        else:
            user_message = PROMPT_NO_QUESTION_PLUS.format(
                paragraph=row["paragraph"],
                question=row["question"],
                choices=choices_string,
            )

        test_dataset.append(
            {
                "id": row["id"],
                "messages": [
                    {"role": "system", "content": system_message},
                    {"role": "user", "content": user_message},
                ],
            }
        )
    return test_dataset

# Inference with Zero-Shot CoT
test_df = load_and_process_data(inference_csv)
test_dataset = format_inference_dataset(test_df)

infer_results = []

model.eval()
with torch.inference_mode():
    for data in tqdm(test_dataset, total=len(test_dataset)):
        input_tensor = tokenizer.apply_chat_template(
            data["messages"],
            tokenize=True,
            add_generation_prompt=True,
            return_tensors="pt",
        ).to("cuda")

        outputs = model.generate(
            input_tensor,
            max_new_tokens=150,  # CoT를 위한 충분한 출력 길이
            pad_token_id=tokenizer.eos_token_id,   
            eos_token_id=tokenizer.eos_token_id,
            do_sample=False,
        )

        response = tokenizer.decode(outputs[0][input_tensor.size(1):], skip_special_tokens=True).strip()

        # Extract the answer using a regular expression
        match = re.search(r'Answer:\s*(\d)', response)
        answer = match.group(1) if match else None

        infer_results.append({
            "id": data["id"],
            "answer": answer,
        })

# Save Results
pd.DataFrame(infer_results).to_csv(output_csv, index=False)
