{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ffa4da9-743e-4c7c-bfa6-90b8e7c66f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5eaf7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bitsandbytes 안 깔려있으면 주석 해제 후 설치\n",
    "# !pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96432bf4-7852-4dcc-9e63-0ba3b6da927e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = 'Bllossom/llama-3.2-Korean-Bllossom-3B'\n",
    "inference_csv = 'data/test.csv'\n",
    "output_csv = 'zeroshot_result/output_change_system_m.csv'\n",
    "\n",
    "system_message = \"\"\"당신은 객관식 문제의 정답을 찾는 데 특화된 AI입니다.\n",
    "- 지문을 분석해 가장 적합한 정답을 찾아야 합니다.\n",
    "- 정답은 **숫자(1, 2, 3, 4, 5)** 형식으로만 작성합니다.\n",
    "- 정답 외의 어떤 설명도 포함하지 마세요.\"\"\"\n",
    "\n",
    "PROMPT_NO_QUESTION_PLUS = \"\"\"\n",
    "정답은 반드시 숫자(1, 2, 3, 4, 5)로만 적어야합니다. 예시) 정답: 1\n",
    "\n",
    "지문:\n",
    "{paragraph}\n",
    "\n",
    "질문:\n",
    "{question}\n",
    "\n",
    "선택지:\n",
    "{choices}\n",
    "\n",
    "\n",
    "정답:\"\"\"\n",
    "\n",
    "PROMPT_QUESTION_PLUS = \"\"\"\n",
    "정답은 반드시 숫자(1, 2, 3, 4, 5)로만 적어야합니다. 예시) 정답: 1\n",
    "\n",
    "지문:\n",
    "{paragraph}\n",
    "\n",
    "질문:\n",
    "{question}\n",
    "\n",
    "<보기>:\n",
    "{question_plus}\n",
    "\n",
    "선택지:\n",
    "{choices}\n",
    "\n",
    "\n",
    "정답:\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b647fb0-abb6-4d34-bbcf-78ed7f7f1cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8399d1ba73784d8fae7dbfe934a00b65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# torch.bfloat16과 load_in_4bit 중 택 1\n",
    "# bfloat16이 더 빠르나, OOM이 발생할 경우 load_in_4bit 사용\n",
    "# 추론 시, 긴 지문+보기가 나올 경우 OOM이 발생할 수 있음\n",
    "# 둘 중, 하나를 주석 처리해주세요.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path, trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    #torch_dtype=torch.bfloat16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path, trust_remote_code=True,\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42044e1",
   "metadata": {},
   "source": [
    "### ### 이 셀 아래부터는 수정하지 않아도 됩니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c8d6419-ffde-4418-b7cd-c07625346ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_data(file_path):\n",
    "    dataset = pd.read_csv(file_path)\n",
    "    \n",
    "    records = []\n",
    "    for _, row in dataset.iterrows():\n",
    "        problems = literal_eval(row['problems'])\n",
    "        record = {\n",
    "            'id': row['id'],\n",
    "            'paragraph': row['paragraph'],\n",
    "            'question': problems['question'],\n",
    "            'choices': problems['choices'],\n",
    "            'answer': problems.get('answer', None),\n",
    "            \"question_plus\": problems.get('question_plus', None),\n",
    "            'explanation': row.get('explanation', None)\n",
    "        }\n",
    "        records.append(record)\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    df['question_plus'] = df['question_plus'].fillna('')\n",
    "    return Dataset.from_pandas(df)\n",
    "\n",
    "def format_inference_dataset(test_df):\n",
    "    test_dataset = []\n",
    "    for i, row in test_df.iterrows():\n",
    "        choices_string = \"\\n\".join([f\"{idx + 1} - {choice}\" for idx, choice in enumerate(row[\"choices\"])])\n",
    "        len_choices = len(row[\"choices\"])\n",
    "        \n",
    "        # <보기>가 있을 때\n",
    "        if row[\"question_plus\"]:\n",
    "            user_message = PROMPT_QUESTION_PLUS.format(\n",
    "                paragraph=row[\"paragraph\"],\n",
    "                question=row[\"question\"],\n",
    "                question_plus=row[\"question_plus\"],\n",
    "                choices=choices_string,\n",
    "            )\n",
    "        # <보기>가 없을 때\n",
    "        else:\n",
    "            user_message = PROMPT_NO_QUESTION_PLUS.format(\n",
    "                paragraph=row[\"paragraph\"],\n",
    "                question=row[\"question\"],\n",
    "                choices=choices_string,\n",
    "            )\n",
    "\n",
    "        test_dataset.append(\n",
    "            {\n",
    "                \"id\": row[\"id\"],\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": system_message},\n",
    "                    {\"role\": \"user\", \"content\": user_message},\n",
    "                ],\n",
    "                \"label\": row[\"answer\"],\n",
    "                \"len_choices\": len_choices,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "544283d9-55b5-415e-bedb-22e74b121f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You shouldn't move a model that is dispatched using accelerate hooks.\n",
      "  0%|          | 0/869 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:452: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n",
      "100%|██████████| 869/869 [07:25<00:00,  1.95it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model.to(\"cuda\")\n",
    "\n",
    "test_df = load_and_process_data(inference_csv).to_pandas()\n",
    "test_dataset = format_inference_dataset(test_df)  # test_dataset은 list of dict\n",
    "\n",
    "infer_results = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for data in tqdm(test_dataset, total=len(test_dataset)):\n",
    "        input_tensor = tokenizer.apply_chat_template(\n",
    "            data[\"messages\"],\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        outputs = model.generate(\n",
    "            input_tensor,\n",
    "            max_new_tokens=1,\n",
    "            pad_token_id=tokenizer.eos_token_id,   \n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=False,\n",
    "            temperature=None, \n",
    "            top_p=None,\n",
    "            top_k=None,\n",
    "        )\n",
    "\n",
    "        response = tokenizer.decode(outputs[0][input_tensor.size(1):], skip_special_tokens=True)\n",
    "\n",
    "        infer_results.append({\n",
    "            \"id\": data[\"id\"],\n",
    "            \"answer\": response.strip()\n",
    "        })\n",
    "\n",
    "pd.DataFrame(infer_results).to_csv(output_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18d7a50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
