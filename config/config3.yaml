model:
  model_name_or_path: NCSOFT/Llama-VARCO-8B-Instruct # models/bllossom_aug/checkpoint-1400 # Bllossom/llama-3.2-Korean-Bllossom-3B
  load_in_8b: True

data:
  train_csv: data/combined_aug_train_validation_clean.csv
  inference_csv: data/test.csv
  output_csv: outputs/output_varco_aug.csv
  train_test_split_ratio: 0.1
  tokenize_max_length: 1800

train:
  do_train: True
  do_eval: True
  lr_scheduler_type: cosine
  max_seq_length: 1800
  output_dir: models/varco_aug
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 8
  num_train_epochs: 2
  learning_rate: 5e-5
  weight_decay: 0.01
  logging_steps: 20
  save_strategy: steps
  eval_strategy: steps
  eval_steps: 100
  save_steps: 100
  save_total_limit: 5
  save_only_model: True
  fp16: True
  warmup_steps: 500
  report_to: none

seed: 42

peft:
  r: 16
  lora_alpha: 32
  lora_dropout: 0.1
  target_modules: ['q_proj', 'k_proj', 'v_proj']
  bias: none
  task_type: CAUSAL_LM

prompt:
  PROMPT_NO_QUESTION_PLUS: |
    지문:
    {paragraph}

    질문:
    {question}

    선택지:
    {choices}

    1, 2, 3, 4, 5 중에 하나를 정답으로 고르세요.
    정답:
    
  PROMPT_QUESTION_PLUS: |
    지문:
    {paragraph}

    질문:
    {question}

    <보기>:
    {question_plus}

    선택지:
    {choices}

    1, 2, 3, 4, 5 중에 하나를 정답으로 고르세요.
    정답:
  
  tokenizer_chat_template: default
  
  response_template: "<|start_header_id|>assistant<|end_header_id|>\n\n"
  
  # evaluation.py의 compute_metrics에서 정답 분리할 때 쓰는 compute_metrics_split
  compute_metrics_end_token: "<|eot_id|>"

  system_message: 지문을 읽고 질문의 답을 구하세요.
